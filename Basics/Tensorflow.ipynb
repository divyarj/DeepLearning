{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2325d59d-8e93-4248-9f7b-a9033129126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b23257-c1fa-4480-b38c-fd53302f7924",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love my dog\", \"I love my cat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6295e8-043c-4d15-8eb9-604770a5e8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e88459e5-b890-4867-95ac-6f614b30eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love my dog\", \"I love my cat\", \"you love my dog!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95330752-2185-4da5-93a5-2016b836f590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69addc6c-13c7-412d-993f-52f86d18c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love my dog\", \"I love my cat\", \"you love my dog!\", \"do you think my dog is amazing?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e389453a-35d4-43dc-b6d9-1b698715d018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847b2d1d-095d-4cb4-bd21-3c8728174a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "388ce6e8-f5de-47a7-bc15-ccd34f905a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = [\"I really love my dog\", \"my dog loves my uncle\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b3f563-305c-462e-b996-f25e5645b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer didn't see the words before \n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04a526e-03a5-4c00-a092-fae2ab6e48a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# not to lose the length of the sequence or texts\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fed454c4-0dcd-4432-a2ff-ff292a368143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3daee7e-4c7e-410b-994f-e81449a7d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding to ensure the input is in same dimensions\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f243887-36dc-4fca-9c4b-6e9635d4ea79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  4  2  1  3]\n",
      " [ 0  0  0  4  2  1  6]\n",
      " [ 0  0  0  5  2  1  3]\n",
      " [ 7  5  8  1  3  9 10]]\n"
     ]
    }
   ],
   "source": [
    "# padding = 'post' , maxlen = 5 , truncate = 'post'\n",
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc57669-b8b5-4e03-a3c2-1fb18aa06f2d",
   "metadata": {},
   "source": [
    "Layer By Layer View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d598e5d0-4cb1-48ed-85f4-91037b68fd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I love this movie\", \"This movie is terrible\", \"Best film ever\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f6077ce-e550-4373-9db9-28e801db6edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 1, 'movie': 2, 'i': 3, 'love': 4, 'is': 5, 'terrible': 6, 'best': 7, 'film': 8, 'ever': 9}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f9c06b0-175a-4076-9d19-7040956b190d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 1 2]\n",
      " [1 2 5 6]\n",
      " [0 7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "padded = pad_sequences(sequences)\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acab325c-53ec-4606-b7bc-0cb3ab58ad02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Parameters\n",
    "vocab_size = 10  # Vocabulary size + 1 for padding (index 0)\n",
    "embedding_dim = 4  # Dimension of the embedding vectors\n",
    "\n",
    "# Building the model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1d14ce6-e1b3-493d-8100-e3512617f496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequences:\n",
      " [[3 4 1 2]\n",
      " [1 2 5 6]\n",
      " [0 7 8 9]]\n",
      "\n",
      "Embedding output:\n",
      " [[[-0.00515059 -0.04360912 -0.03211959  0.00638112]\n",
      "  [ 0.01303181 -0.02852996 -0.03487139 -0.02223028]\n",
      "  [ 0.02221173 -0.03948899 -0.00527262  0.00718025]\n",
      "  [ 0.04398812 -0.00190734  0.03003671 -0.0405668 ]]\n",
      "\n",
      " [[ 0.02221173 -0.03948899 -0.00527262  0.00718025]\n",
      "  [ 0.04398812 -0.00190734  0.03003671 -0.0405668 ]\n",
      "  [-0.02892005  0.03679902 -0.04293055  0.00268785]\n",
      "  [ 0.01666312  0.03365344  0.0338567   0.01567891]]\n",
      "\n",
      " [[-0.00232523 -0.04276546  0.00192988 -0.00284358]\n",
      "  [ 0.01780419  0.00768463  0.01692016 -0.04765263]\n",
      "  [-0.02265648 -0.0179026   0.02033291 -0.03539554]\n",
      "  [ 0.02030182 -0.02486253 -0.0161811  -0.00883853]]]\n",
      "\n",
      "Pooled output:\n",
      " [[ 0.01852027 -0.02838385 -0.01055672 -0.01230893]\n",
      " [ 0.01348573  0.00726403  0.00392256 -0.00375495]\n",
      " [ 0.00328108 -0.01946149  0.00575046 -0.02368257]]\n",
      "\n",
      "Dense output:\n",
      " [[0.5012482 ]\n",
      " [0.49754938]\n",
      " [0.5057437 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Getting the output of each layer for the example input\n",
    "embedding_layer = model.layers[0]\n",
    "pooling_layer = model.layers[1]\n",
    "dense_layer = model.layers[2]\n",
    "\n",
    "# Example input sequences\n",
    "padded_sequences = tf.constant(padded)\n",
    "\n",
    "# Forward pass\n",
    "embedding_output = embedding_layer(padded_sequences)\n",
    "pooled_output = pooling_layer(embedding_output)\n",
    "dense_output = dense_layer(pooled_output)\n",
    "\n",
    "# Printing the outputs\n",
    "print(\"Input sequences:\\n\", padded_sequences.numpy())\n",
    "print(\"\\nEmbedding output:\\n\", embedding_output.numpy())\n",
    "print(\"\\nPooled output:\\n\", pooled_output.numpy())\n",
    "print(\"\\nDense output:\\n\", dense_output.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2ab8f-0148-457c-b08c-7ad798202b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
